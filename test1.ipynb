{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5637a056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated code for enhanced RAG system with semantic chunking, metadata tagging, and hybrid retrieval\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import hashlib\n",
    "import fitz\n",
    "import uuid\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "from dotenv import load_dotenv\n",
    "from typing import List\n",
    "from langchain_core.documents import Document\n",
    "from langchain.text_splitter import SemanticChunker\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.embeddings import AzureOpenAIEmbeddings\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "\n",
    "# Load env\n",
    "load_dotenv()\n",
    "\n",
    "# Tesseract path (Windows)\n",
    "pytesseract.pytesseract.tesseract_cmd = r\"C:\\\\Program Files\\\\Tesseract-OCR\\\\tesseract.exe\"\n",
    "\n",
    "# Azure config\n",
    "AZURE_OPENAI_API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "AZURE_OPENAI_API_VERSION = os.getenv(\"AZURE_OPENAI_API_VERSION\")\n",
    "EMBEDDING_DEPLOYMENT = os.getenv(\"AZURE_OPENAI_EMBED_DEPLOYMENT\")\n",
    "LLM_DEPLOYMENT = os.getenv(\"AZURE_OPENAI_LLM_DEPLOYMENT\")\n",
    "\n",
    "# Paths\n",
    "PDF_DIR = \"./source_docs\"\n",
    "CHAT_HISTORY_DIR = \"chat_history\"\n",
    "FAISS_INDEX_PATH = \"./store\"\n",
    "HASH_STORE_PATH = \"./hashes/index_hashes.txt\"\n",
    "TEXT_CACHE_DIR = \"./text_cache\"\n",
    "\n",
    "# Embeddings\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    azure_deployment=EMBEDDING_DEPLOYMENT,\n",
    "    openai_api_key=AZURE_OPENAI_API_KEY,\n",
    "    openai_api_version=AZURE_OPENAI_API_VERSION,\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT\n",
    ")\n",
    "\n",
    "def file_hash(filepath):\n",
    "    h = hashlib.sha256()\n",
    "    with open(filepath, 'rb') as f:\n",
    "        while chunk := f.read(8192):\n",
    "            h.update(chunk)\n",
    "    return h.hexdigest()\n",
    "\n",
    "def load_existing_hashes():\n",
    "    if not os.path.exists(HASH_STORE_PATH): return set()\n",
    "    with open(HASH_STORE_PATH, \"r\") as f:\n",
    "        return set(line.strip() for line in f)\n",
    "\n",
    "def save_hashes(hashes: set):\n",
    "    with open(HASH_STORE_PATH, \"w\") as f:\n",
    "        for h in sorted(hashes): f.write(f\"{h}\\n\")\n",
    "\n",
    "def extract_text_with_ocr(pdf_path):\n",
    "    filename = os.path.basename(pdf_path)\n",
    "    md_path = os.path.join(TEXT_CACHE_DIR, filename.replace(\".pdf\", \".md\"))\n",
    "    if os.path.exists(md_path):\n",
    "        with open(md_path, \"r\", encoding=\"utf-8\") as f: return f.read()\n",
    "\n",
    "    doc = fitz.open(pdf_path)\n",
    "    full_text = \"\"\n",
    "    for page_num in range(len(doc)):\n",
    "        page = doc.load_page(page_num)\n",
    "        text = page.get_text()\n",
    "        full_text += f\"\\n\\nPage {page_num+1}:\\n{text.strip()}\"\n",
    "\n",
    "        if not text.strip():\n",
    "            try:\n",
    "                pix = page.get_pixmap(dpi=300)\n",
    "                image = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
    "                ocr_text = pytesseract.image_to_string(image)\n",
    "                full_text += f\"\\nOCR:\\n{ocr_text.strip()}\"\n",
    "            except Exception as e:\n",
    "                print(f\"OCR failed on page {page_num + 1}: {e}\")\n",
    "\n",
    "    os.makedirs(TEXT_CACHE_DIR, exist_ok=True)\n",
    "    with open(md_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(full_text)\n",
    "    return full_text\n",
    "\n",
    "def enrich_metadata(filename):\n",
    "    year_match = re.search(r\"(20\\\\d{2})\", filename)\n",
    "    return {\n",
    "        \"source\": filename,\n",
    "        \"year\": year_match.group(1) if year_match else \"Unknown\",\n",
    "        \"fund\": \"UTF\",\n",
    "        \"doc_type\": \"Annual Report\"\n",
    "    }\n",
    "\n",
    "def update_faiss_index(embeddings):\n",
    "    existing_hashes = load_existing_hashes()\n",
    "    new_hashes = set()\n",
    "    new_docs = []\n",
    "\n",
    "    for filename in os.listdir(PDF_DIR):\n",
    "        if not filename.lower().endswith(\".pdf\"): continue\n",
    "        pdf_path = os.path.join(PDF_DIR, filename)\n",
    "        h = file_hash(pdf_path)\n",
    "        if h in existing_hashes:\n",
    "            print(f\"Skipping already indexed: {filename}\")\n",
    "            continue\n",
    "\n",
    "        text = extract_text_with_ocr(pdf_path)\n",
    "        metadata = enrich_metadata(filename)\n",
    "        new_docs.append(Document(page_content=text, metadata=metadata))\n",
    "        new_hashes.add(h)\n",
    "\n",
    "    if not new_docs:\n",
    "        return FAISS.load_local(FAISS_INDEX_PATH, embeddings, allow_dangerous_deserialization=True)\n",
    "\n",
    "    splitter = SemanticChunker(embeddings, chunk_size=1000)\n",
    "    chunks = splitter.split_documents(new_docs)\n",
    "\n",
    "    if os.path.exists(FAISS_INDEX_PATH + \".faiss\"):\n",
    "        vs = FAISS.load_local(FAISS_INDEX_PATH, embeddings, allow_dangerous_deserialization=True)\n",
    "        vs.add_documents(chunks)\n",
    "    else:\n",
    "        vs = FAISS.from_documents(chunks, embeddings)\n",
    "    vs.save_local(FAISS_INDEX_PATH)\n",
    "\n",
    "    updated_hashes = existing_hashes.union(new_hashes)\n",
    "    save_hashes(updated_hashes)\n",
    "    return vs\n",
    "\n",
    "def load_or_create_vectorstore(embeddings):\n",
    "    return update_faiss_index(embeddings)\n",
    "\n",
    "class PersistentChatMessageHistory(ChatMessageHistory):\n",
    "    def __init__(self, session_id):\n",
    "        super().__init__()\n",
    "        self._session_id = session_id\n",
    "        self._file_path = os.path.join(CHAT_HISTORY_DIR, f\"{session_id}.json\")\n",
    "        self.load()\n",
    "    def load(self):\n",
    "        if os.path.exists(self._file_path):\n",
    "            with open(self._file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                raw = json.load(f)\n",
    "                self.messages = [self._dict_to_message(msg) for msg in raw]\n",
    "    def save(self):\n",
    "        with open(self._file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump([self._message_to_dict(m) for m in self.messages], f, indent=2)\n",
    "    def add_message(self, m):\n",
    "        super().add_message(m)\n",
    "        self.save()\n",
    "    def _message_to_dict(self, m):\n",
    "        return {\"type\": m.type, \"content\": m.content}\n",
    "    def _dict_to_message(self, d):\n",
    "        from langchain_core.messages import HumanMessage, AIMessage\n",
    "        return HumanMessage(content=d[\"content\"]) if d[\"type\"] == \"human\" else AIMessage(content=d[\"content\"])\n",
    "\n",
    "def setup_rag_chain_with_history(session_id, embeddings):\n",
    "    vectorstore = load_or_create_vectorstore(embeddings)\n",
    "    retriever = vectorstore.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 10, \"fetch_k\": 30})\n",
    "\n",
    "    llm = AzureChatOpenAI(\n",
    "        deployment_name=LLM_DEPLOYMENT,\n",
    "        api_key=AZURE_OPENAI_API_KEY,\n",
    "        azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "        api_version=AZURE_OPENAI_API_VERSION,\n",
    "        temperature=0.3\n",
    "    )\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\",\n",
    "         \"You are a development results analyst AI assistant.\\n\\n\"\n",
    "         \"Extract and summarize **results stories** from UTF annual reports.\\n\\n\"\n",
    "         \"**üìå Title:** Bold, 5‚Äì10 words\\n\"\n",
    "         \"**üìù Summary:** 4‚Äì6 sentences\\n\"\n",
    "         \"**üóÇ Metadata:** Region, Sector, Donor, Source Document & Page\\n\\n\"\n",
    "         \"Respond with structured stories if available, else fallback to answering user's question from context.\\n\"\n",
    "         \"Never invent facts.\\n\\nContext:\\n{context}\"),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{input}\")\n",
    "    ])\n",
    "\n",
    "    doc_chain = create_stuff_documents_chain(llm=llm, prompt=prompt)\n",
    "    rag_chain = create_retrieval_chain(retriever, doc_chain)\n",
    "\n",
    "    return RunnableWithMessageHistory(\n",
    "        rag_chain,\n",
    "        lambda session_id: PersistentChatMessageHistory(session_id),\n",
    "        input_messages_key=\"input\",\n",
    "        history_messages_key=\"chat_history\",\n",
    "        output_messages_key=\"answer\"\n",
    "    )\n",
    "\n",
    "def run_query(session_id: str, question: str):\n",
    "    rag_chain = setup_rag_chain_with_history(session_id, embeddings)\n",
    "    result = rag_chain.invoke({\"input\": question}, config={\"configurable\": {\"session_id\": session_id}})\n",
    "    return result[\"answer\"]\n",
    "\n",
    "# Sample run\n",
    "if __name__ == \"__main__\":\n",
    "    sid = f\"session_{uuid.uuid4().hex[:8]}\"\n",
    "    q = \"Give me two examples of how the MDTF supported private sector job creation in 2020\"\n",
    "    print(\"\\nQuery:\", q)\n",
    "    print(\"\\nAnswer:\\n\", run_query(sid, q))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
