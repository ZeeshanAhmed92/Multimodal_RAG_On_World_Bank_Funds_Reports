{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9708e6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import pickle\n",
    "import fitz\n",
    "import uuid\n",
    "import hashlib\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "from typing import List\n",
    "from dotenv import load_dotenv  \n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain.embeddings import AzureOpenAIEmbeddings\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d523803e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load .env file for Azure keys/config\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d65e9995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Set path to tesseract executable on Windows\n",
    "pytesseract.pytesseract.tesseract_cmd = r\"C:\\Program Files\\Tesseract-OCR\\tesseract.exe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "dddf996c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Azure OpenAI config\n",
    "AZURE_OPENAI_API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "AZURE_OPENAI_API_VERSION = os.getenv(\"AZURE_OPENAI_API_VERSION\")\n",
    "EMBEDDING_DEPLOYMENT = os.getenv(\"AZURE_OPENAI_EMBED_DEPLOYMENT\")  # e.g. text-embedding-3-small\n",
    "LLM_DEPLOYMENT = os.getenv(\"AZURE_OPENAI_LLM_DEPLOYMENT\")          # e.g. gpt-4-mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d8e37895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "# OPENAI_EMBEDDING_MODEL = os.getenv(\"OPENAI_EMBEDDING_MODEL\") \n",
    "# OPENAI_MODEL = os.getenv(\"OPENAI_MODEL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1401f70d",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Setup Azure Embeddings & LLM\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    azure_deployment=EMBEDDING_DEPLOYMENT,\n",
    "    openai_api_key=AZURE_OPENAI_API_KEY,\n",
    "    openai_api_version=AZURE_OPENAI_API_VERSION,\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    chunk_size=1000,  # ‚úÖ \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ceeda2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings = OpenAIEmbeddings(\n",
    "#     model=OPENAI_EMBEDDING_MODEL,\n",
    "#     openai_api_key=OPENAI_API_KEY\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "bb0701b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")  # Or \"all-minilm\" or \"bge-base-en\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2c80cb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Path Configs ===\n",
    "PDF_DIR = \"./source_docs\"\n",
    "CHAT_HISTORY_DIR = \"chat_history\"\n",
    "FAISS_INDEX_PATH = \"./store\"  # ‚úÖ Now points directly to where index.faiss is\n",
    "METADATA_STORE_PATH = \"./store/index.pkl\"  # ‚úÖ Points to the actual pickle file\n",
    "HASH_STORE_PATH = \"./hashes/index_hashes.txt\"\n",
    "TEXT_CACHE_DIR = \"./text_cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c4e3f59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_with_ocr(pdf_path):\n",
    "    filename = os.path.basename(pdf_path)\n",
    "    md_filename = os.path.splitext(filename)[0] + \".md\"\n",
    "    md_path = os.path.join(TEXT_CACHE_DIR, md_filename)\n",
    "\n",
    "    # If cached .md file exists, read it\n",
    "    if os.path.exists(md_path):\n",
    "        print(f\"üìÑ Cached text found for {filename}, loading from Markdown.\")\n",
    "        with open(md_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            return f.read()\n",
    "\n",
    "    print(f\"üîç OCR processing: {filename}\")\n",
    "    full_text = \"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "\n",
    "    for page_num in range(len(doc)):\n",
    "        page = doc.load_page(page_num)\n",
    "        text = page.get_text()\n",
    "        full_text += f\"\\n\\n## Page {page_num + 1} Text\\n{text.strip()}\"\n",
    "\n",
    "        try:\n",
    "            pix = page.get_pixmap(dpi=300)\n",
    "            image = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
    "            ocr_text = pytesseract.image_to_string(image)\n",
    "            full_text += f\"\\n\\n## Page {page_num + 1} OCR\\n{ocr_text.strip()}\"\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è OCR failed on page {page_num + 1}: {e}\")\n",
    "\n",
    "    # Save as markdown in readable form\n",
    "    os.makedirs(TEXT_CACHE_DIR, exist_ok=True)\n",
    "    with open(md_path, \"w\", encoding=\"utf-8\") as f_md:\n",
    "        f_md.write(full_text)\n",
    "\n",
    "    return full_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0c94a702",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_year(filename):\n",
    "    match = re.search(r\"(20\\d{2})\", filename)\n",
    "    return match.group(1) if match else \"Unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ef51dd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_hash(filepath):\n",
    "    \"\"\"Generate SHA256 hash of a file.\"\"\"\n",
    "    h = hashlib.sha256()\n",
    "    with open(filepath, 'rb') as f:\n",
    "        while chunk := f.read(8192):\n",
    "            h.update(chunk)\n",
    "    return h.hexdigest()\n",
    "\n",
    "def load_existing_hashes():\n",
    "    \"\"\"Load file hashes from index_hashes.txt.\"\"\"\n",
    "    if not os.path.exists(HASH_STORE_PATH):\n",
    "        return set()\n",
    "    with open(HASH_STORE_PATH, \"r\") as f:\n",
    "        return set(line.strip() for line in f.readlines())\n",
    "\n",
    "def save_hashes(hashes: set):\n",
    "    \"\"\"Save updated hashes to index_hashes.txt.\"\"\"\n",
    "    with open(HASH_STORE_PATH, \"w\") as f:\n",
    "        for h in sorted(hashes):\n",
    "            f.write(f\"{h}\\n\")\n",
    "\n",
    "def enrich_metadata(filename: str) -> dict:\n",
    "    year_match = re.search(r\"(20\\d{2})\", filename)\n",
    "    return {\n",
    "        \"source\": filename,\n",
    "        \"year\": year_match.group(1) if year_match else \"Unknown\",\n",
    "        \"fund\": \"UTF\",\n",
    "        \"doc_type\": \"Annual Report\"\n",
    "    }\n",
    "\n",
    "def update_faiss_index(embeddings):\n",
    "    print(\"üîÑ Checking for new documents...\")\n",
    "    \n",
    "    # Load known hashes\n",
    "    existing_hashes = load_existing_hashes()\n",
    "    new_hashes = set()\n",
    "    new_documents = []\n",
    "\n",
    "    for filename in os.listdir(PDF_DIR):\n",
    "        if not filename.lower().endswith(\".pdf\"):\n",
    "            continue\n",
    "\n",
    "        pdf_path = os.path.join(PDF_DIR, filename)\n",
    "        file_digest = file_hash(pdf_path)\n",
    "\n",
    "        if file_digest in existing_hashes:\n",
    "            print(f\"‚è≠Ô∏è Skipping already indexed: {filename}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"üìÑ New PDF detected: {filename}\")\n",
    "        text = extract_text_with_ocr(pdf_path)\n",
    "        metadata = enrich_metadata(filename)\n",
    "        new_documents.append(Document(page_content=text, metadata=metadata))\n",
    "        new_hashes.add(file_digest)\n",
    "\n",
    "    # No new docs? Load and return existing vector store\n",
    "    if not new_documents:\n",
    "        print(\"‚úÖ No new documents found.\")\n",
    "        return FAISS.load_local(FAISS_INDEX_PATH, embeddings, allow_dangerous_deserialization=True)\n",
    "\n",
    "    print(\"‚úÇÔ∏è Splitting documents...\")\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=800,\n",
    "        chunk_overlap=100,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \",\", \" \", \"\"]\n",
    "    )\n",
    "    new_chunks = splitter.split_documents(new_documents)\n",
    "\n",
    "    print(\"üì¶ Updating FAISS vector store...\")\n",
    "    if os.path.exists(FAISS_INDEX_PATH + \".faiss\"):\n",
    "        vectorstore = FAISS.load_local(FAISS_INDEX_PATH, embeddings, allow_dangerous_deserialization=True)\n",
    "        vectorstore.add_documents(new_chunks)\n",
    "    else:\n",
    "        vectorstore = FAISS.from_documents(new_chunks, embeddings)\n",
    "\n",
    "    vectorstore.save_local(FAISS_INDEX_PATH)\n",
    "\n",
    "    # Save combined hashes\n",
    "    updated_hashes = existing_hashes.union(new_hashes)\n",
    "    save_hashes(updated_hashes)\n",
    "    print(f\"‚úÖ Stored {len(updated_hashes)} file hashes in {HASH_STORE_PATH}\")\n",
    "\n",
    "    return vectorstore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5fd097e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_or_create_vectorstore(embeddings):\n",
    "    return update_faiss_index(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6db16815",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PersistentChatMessageHistory(ChatMessageHistory):\n",
    "    def __init__(self, session_id: str):\n",
    "        super().__init__()\n",
    "        self._session_id = session_id\n",
    "        self._file_path = os.path.join(CHAT_HISTORY_DIR, f\"{session_id}.json\")\n",
    "        self.load()\n",
    "\n",
    "    def load(self):\n",
    "        if os.path.exists(self._file_path):\n",
    "            with open(self._file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                raw = json.load(f)\n",
    "                self.messages = [self._dict_to_message(msg) for msg in raw]\n",
    "\n",
    "    def save(self):\n",
    "        with open(self._file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump([self._message_to_dict(msg) for msg in self.messages], f, indent=2)\n",
    "\n",
    "    def add_message(self, message):\n",
    "        super().add_message(message)\n",
    "        self.save()\n",
    "\n",
    "    def _message_to_dict(self, message):\n",
    "        return {\"type\": message.type, \"content\": message.content}   \n",
    "\n",
    "    def _dict_to_message(self, data):\n",
    "        from langchain_core.messages import HumanMessage, AIMessage\n",
    "        return HumanMessage(content=data[\"content\"]) if data[\"type\"] == \"human\" else AIMessage(content=data[\"content\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a136b9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Create RAG Chain with Story Extraction Prompt ===\n",
    "def setup_rag_chain_with_history(session_id: str, embeddings):\n",
    "    vectorstore = load_or_create_vectorstore(embeddings)\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": 7})\n",
    "\n",
    "    # llm = ChatOpenAI(\n",
    "    # model=OPENAI_MODEL,\n",
    "    # temperature=0,\n",
    "    # openai_api_key=OPENAI_API_KEY\n",
    "    # )\n",
    "    # llm = Ollama(model=\"llama3.2:latest\")  # or any model like \"mistral\", \"phi3\", etc.\n",
    "    llm = AzureChatOpenAI(\n",
    "        deployment_name=LLM_DEPLOYMENT,\n",
    "        api_key=AZURE_OPENAI_API_KEY,\n",
    "        azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "        api_version=AZURE_OPENAI_API_VERSION,\n",
    "        temperature=0.3\n",
    "    )\n",
    "\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \n",
    "     \"You are an AI assistant helping users retrieve development results from UTF annual reports.\\n\\n\"\n",
    "     \"Your main goal is to extract and summarize *results stories* when possible.\\n\\n\"\n",
    "     \"Each results story should include:\\n\"\n",
    "     \"1. A Bold short, descriptive title (5‚Äì10 words)\\n\"\n",
    "     \"2. A summary of the outcome or impact (5‚Äì6 sentences) with bold summary title\\n\"\n",
    "     \"3. Structured metadata:\\n\"\n",
    "     \"   - **Region**\\n\"\n",
    "     \"   - **Sector**\\n\"\n",
    "     \"   - **Donor/Fund**\\n\"\n",
    "     \"   - **Source Document and Page**\\n\\n\"\n",
    "     \"üëâ If you **find stories** related to the user‚Äôs question, present them in the structured format above. Make proper headings and make them bold, dont put ## instead of making bold\\n\"\n",
    "     \"üëâ If **no full stories** are available, **fallback to answering the user's question** based on the relevant context from the document.\\n\\n\"\n",
    "     \"Be clear and informative. Never make up facts.\\n\\n\"\n",
    "     \"Context:\\n{context}\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "    document_chain = create_stuff_documents_chain(llm=llm, prompt=prompt)\n",
    "    rag_chain = create_retrieval_chain(retriever, document_chain)\n",
    "\n",
    "    return RunnableWithMessageHistory(\n",
    "        rag_chain,\n",
    "        lambda session_id: PersistentChatMessageHistory(session_id),\n",
    "        input_messages_key=\"input\",\n",
    "        history_messages_key=\"chat_history\",\n",
    "        output_messages_key=\"answer\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "40d96ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Run a Query ===\n",
    "def run_query(session_id: str, question: str):\n",
    "    rag_chain = setup_rag_chain_with_history(session_id, embeddings)\n",
    "    result = rag_chain.invoke(\n",
    "        {\"input\": question},\n",
    "        config={\"configurable\": {\"session_id\": session_id}}\n",
    "    )\n",
    "    return result[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3e172b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " give me two examples of how the MDTF supported private sector job creation in 2020\n",
      "üîÑ Checking for new documents...\n",
      "‚è≠Ô∏è Skipping already indexed: 2020TrustFundAnnualReports.pdf\n",
      "‚è≠Ô∏è Skipping already indexed: 2021TrustFundAnnualReports.pdf\n",
      "‚è≠Ô∏è Skipping already indexed: 2022TrustFundAnnualReports.pdf\n",
      "‚è≠Ô∏è Skipping already indexed: 2023TrustFundAnnualReports.pdf\n",
      "‚è≠Ô∏è Skipping already indexed: 2024TrustFundAnnualReports.pdf\n",
      "‚úÖ No new documents found.\n",
      "üß† **Example 1: IFC‚Äôs Fast-Track COVID-19 Facility Supporting Private Sector Jobs**\n",
      "\n",
      "**Summary:**  \n",
      "In 2020, the IFC launched its Fast-Track COVID-19 Facility, which increased to $8.6 billion, to support private sector job creation during the pandemic. The facility provided $7.4 billion to finance 103 projects that offered liquidity, working capital, and trade financing to keep companies operational, especially in industries most affected by COVID-19. This initiative included a Base of the Pyramid Program aimed at supporting the poorest and hardest-hit populations, initially launched with $400 million and later receiving an additional $200 million in 2022. By maintaining business operations, the facility helped preserve jobs and stabilize economic activity in fragile and conflict-affected areas. This approach demonstrated how targeted financial support can sustain employment during crises.\n",
      "\n",
      "- **Region:** Global (including fragile and conflict-affected areas)  \n",
      "- **Sector:** Private Sector / Job Creation  \n",
      "- **Donor/Fund:** IFC Fast-Track COVID-19 Facility  \n",
      "- **Source Document and Page:** WBG Trust Fund Annual Report 2022, page 44-45  \n",
      "\n",
      "---\n",
      "\n",
      "**Example 2: World Bank Emergency Cash Transfers to Support Vulnerable Workers**\n",
      "\n",
      "**Summary:**  \n",
      "The MDTF supported private sector job creation indirectly by funding emergency cash transfer programs in countries like Bangladesh and Colombia during 2020. In Bangladesh, the Human Capital Umbrella Program helped integrate economic inclusion components into major cash transfer schemes, enabling low-income wage earners to better secure employment post-pandemic. In Colombia, grant financing from the State and Peacebuilding Fund (SPF) provided emergency cash transfers to vulnerable migrants and Venezuelan refugees, mitigating COVID-19 spread and easing social tensions with host communities. These interventions helped stabilize vulnerable populations‚Äô livelihoods, supporting their continued participation in local labor markets and private sector activities.\n",
      "\n",
      "- **Region:** Bangladesh and Colombia  \n",
      "- **Sector:** Social Protection / Private Sector Employment Support  \n",
      "- **Donor/Fund:** Human Capital Umbrella Program; State and Peacebuilding Fund (SPF)  \n",
      "- **Source Document and Page:** WBG Trust Fund Annual Report 2022, page 44-45\n"
     ]
    }
   ],
   "source": [
    "session_id = f\"session_{uuid.uuid4().hex[:8]}\"\n",
    "q = \"give me two examples of how the MDTF supported private sector job creation in 2020\" \n",
    "\n",
    "print(f\"\\n {q}\")\n",
    "answer = run_query(session_id, q)\n",
    "print(f\"üß† {answer}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
