{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9708e6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import re\n",
    "import json\n",
    "import base64\n",
    "import hashlib\n",
    "import fitz\n",
    "import uuid\n",
    "import platform\n",
    "import pickle\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "from openai import OpenAI\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "import pymupdf4llm\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain_core.messages import HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d523803e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load .env file for Azure keys/config\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d65e9995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Set path to tesseract executable on Windows\n",
    "pytesseract.pytesseract.tesseract_cmd = r\"C:\\Program Files\\Tesseract-OCR\\tesseract.exe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1633a917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI config\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "OPENAI_API_MODEL=\"gpt-4.1-mini\"\n",
    "OPENAI_EMBEDDING_MODEL=\"text-embedding-3-small\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "05fe43e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "  # Azure OpenAI LLM\n",
    "llm = ChatOpenAI(\n",
    "    model=OPENAI_API_MODEL,\n",
    "    temperature=0.3,\n",
    "    openai_api_key=OPENAI_API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1401f70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  # Setup Azure Embeddings & LLM\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    openai_api_key=OPENAI_API_KEY,\n",
    "    model=OPENAI_EMBEDDING_MODEL,\n",
    "    chunk_size=1000  # Optional: controls how many docs are embedded per batch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2c80cb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Path Configs ===\n",
    "PDF_DIR = \"./source_docs\"\n",
    "FAISS_INDEX_PATH = \"./store\"  # ‚úÖ Now points directly to where index.faiss is\n",
    "METADATA_STORE_PATH = \"./store/index.pkl\"  # ‚úÖ Points to the actual pickle file\n",
    "HASH_STORE_PATH = \"./hashes/pdf_hashes.txt\"\n",
    "output_dir = \"./markdowns\"\n",
    "COMBINED_DIR = \"./combined\"\n",
    "COMBINED_MD_PATH = \"./combined/combined.md\"\n",
    "COMBINED_MD_HASH_STORE = \"./hashes/combined_md_hash.txt\"\n",
    "image_dir = os.path.join(output_dir, \"images\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(image_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9ed3a3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_hash(filepath):\n",
    "    h = hashlib.sha256()\n",
    "    with open(filepath, \"rb\") as f:\n",
    "        while chunk := f.read(8192):\n",
    "            h.update(chunk)\n",
    "    return h.hexdigest()\n",
    "\n",
    "def load_pdf_hashes():\n",
    "    if not os.path.exists(HASH_STORE_PATH):\n",
    "        return set()\n",
    "    with open(HASH_STORE_PATH, \"r\") as f:\n",
    "        return set(line.strip() for line in f)\n",
    "\n",
    "def save_pdf_hashes(hashes: set):\n",
    "    with open(HASH_STORE_PATH, \"w\") as f:\n",
    "        for h in sorted(hashes):\n",
    "            f.write(f\"{h}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ee2e2f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_tesseract_path():\n",
    "    system = platform.system()\n",
    "    if system == \"Windows\":\n",
    "        # Common default install location‚Äîchange if needed\n",
    "        possible = [\n",
    "            r\"C:\\Program Files\\Tesseract-OCR\\tesseract.exe\",\n",
    "            r\"C:\\Program Files (x86)\\Tesseract-OCR\\tesseract.exe\"\n",
    "        ]\n",
    "        for path in possible:\n",
    "            if os.path.isfile(path):\n",
    "                pytesseract.pytesseract.tesseract_cmd = path\n",
    "                break\n",
    "        else:\n",
    "            raise FileNotFoundError(\"Tesseract not found in default Windows paths.\")\n",
    "    else:\n",
    "        # On Linux or macOS, tesseract should be in PATH\n",
    "        pytesseract.pytesseract.tesseract_cmd = \"tesseract\"\n",
    "\n",
    "    # Optional: verify it's working\n",
    "    try:\n",
    "        version = os.popen(f'\"{pytesseract.pytesseract.tesseract_cmd}\" --version').read()\n",
    "        print(\"‚úîÔ∏è Tesseract detected:\", version.splitlines()[0])\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error verifying Tesseract at '{pytesseract.pytesseract.tesseract_cmd}': {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dcfa1260",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_image(image_path_or_bytes):\n",
    "    configure_tesseract_path()\n",
    "\n",
    "    if isinstance(image_path_or_bytes, bytes):\n",
    "        image = Image.open(io.BytesIO(image_path_or_bytes))\n",
    "    else:\n",
    "        image = Image.open(image_path_or_bytes)\n",
    "\n",
    "    # Step 1: OCR text extraction\n",
    "    ocr_text = pytesseract.image_to_string(image)\n",
    "\n",
    "    # Step 2: Prepare image for OpenAI\n",
    "    buffered = io.BytesIO()\n",
    "    image.save(buffered, format=\"JPEG\")\n",
    "    b64 = base64.b64encode(buffered.getvalue()).decode()\n",
    "    data_uri = f\"data:image/jpeg;base64,{b64}\"\n",
    "\n",
    "    # Step 3: LLM-based image description\n",
    "    client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4.1-mini\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"text\", \"text\": \"Please describe the image's layout, key elements, and any text it contains. This is part of an annual development report.\"},\n",
    "                        {\"type\": \"image_url\", \"image_url\": {\"url\": data_uri}}\n",
    "                    ]\n",
    "                }\n",
    "            ],\n",
    "            max_tokens=300\n",
    "        )\n",
    "        image_description = response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Azure image description failed: {e}\")\n",
    "        image_description = \"(No image description due to policy filter or API error)\"\n",
    "\n",
    "\n",
    "    # Step 4: Combine and return\n",
    "    return f\"{ocr_text.strip()}\\n\\n**Image Description:**\\n{image_description.strip()}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f26c751",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import ImageStat\n",
    "\n",
    "\n",
    "def is_blank_or_low_text(image_bytes, threshold=5):\n",
    "    \"\"\"Return True if image is mostly blank or low-content.\"\"\"\n",
    "    image = Image.open(io.BytesIO(image_bytes)).convert(\"L\")  # grayscale\n",
    "    stat = ImageStat.Stat(image)\n",
    "    return stat.stddev[0] < threshold  # low stddev = low variation = blank\n",
    "\n",
    "# def extract_pdf_as_markdown(file_path, base_filename, image_dir):\n",
    "#     doc = fitz.open(file_path)\n",
    "#     markdown = \"\"\n",
    "\n",
    "#     for page_num, page in enumerate(doc):\n",
    "#         markdown += f\"\\n## Page {page_num + 1}\\n\"\n",
    "#         markdown += page.get_text(\"text\") + \"\\n\"\n",
    "\n",
    "#         for img_index, img in enumerate(page.get_images(full=True)):\n",
    "#             xref = img[0]\n",
    "#             base_image = doc.extract_image(xref)\n",
    "#             image_bytes = base_image[\"image\"]\n",
    "#             ext = base_image[\"ext\"]\n",
    "\n",
    "#             image_filename = f\"{base_filename}_p{page_num+1}_img{img_index+1}.{ext}\"\n",
    "#             image_path = os.path.join(image_dir, image_filename)\n",
    "\n",
    "#             if os.path.exists(image_path):\n",
    "#                 print(f\"üîÅ Skipping already processed image: {image_filename}\")\n",
    "#                 continue\n",
    "\n",
    "#             # Skip low-content images\n",
    "#             if is_blank_or_low_text(image_bytes):\n",
    "#                 print(f\"üö´ Skipping blank/low-content image: {image_filename}\")\n",
    "#                 continue\n",
    "\n",
    "#             # Save image\n",
    "#             with open(image_path, \"wb\") as f:\n",
    "#                 f.write(image_bytes)\n",
    "\n",
    "#             # OCR + Vision\n",
    "#             ocr_text = extract_text_from_image(image_bytes)\n",
    "\n",
    "#             markdown += f\"\\n**Image {img_index + 1} OCR + Description:**\\n```\\n{ocr_text.strip()}\\n```\\n\"\n",
    "#             markdown += f\"![Image {img_index + 1}](images/{image_filename})\\n\"\n",
    "\n",
    "#     return markdown\n",
    "def extract_pdf_as_markdown(input_pdf, image_dir=\"images\"):\n",
    "    os.makedirs(image_dir, exist_ok=True)\n",
    "    pages = pymupdf4llm.to_markdown(\n",
    "        input_pdf,\n",
    "        write_images=True,\n",
    "        image_path=image_dir,\n",
    "        dpi=300,\n",
    "        page_chunks=True\n",
    "    )\n",
    "\n",
    "    md = \"\"\n",
    "    for i, page in enumerate(pages):\n",
    "        md += f\"## Page {i+1}\\n\\n\"\n",
    "        md += page[\"text\"] + \"\\n\"  # rich Markdown with tables & image links\n",
    "\n",
    "        for img_meta in page[\"images\"]:\n",
    "            img_path = os.path.join(image_dir, img_meta[\"filename\"])\n",
    "            # Custom OCR function on raw image bytes\n",
    "            ocr = extract_text_from_image(open(img_path, \"rb\").read())\n",
    "            md += f\"\\n**OCR ({img_meta['filename']}):**\\n```\\n{ocr.strip()}\\n```\\n\"\n",
    "\n",
    "    return md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c882b3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_pdfs_to_markdown(pdf_dir=PDF_DIR, output_dir=\"./markdowns\"):\n",
    "    image_dir = os.path.join(output_dir, \"images\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    os.makedirs(image_dir, exist_ok=True)\n",
    "\n",
    "    existing_hashes = load_pdf_hashes()\n",
    "    new_hashes = set()\n",
    "\n",
    "    pdf_files = [f for f in os.listdir(pdf_dir) if f.lower().endswith(\".pdf\")]\n",
    "\n",
    "    for pdf_file in pdf_files:\n",
    "        pdf_path = os.path.join(pdf_dir, pdf_file)\n",
    "        pdf_digest = file_hash(pdf_path)\n",
    "\n",
    "        if pdf_digest in existing_hashes:\n",
    "            print(f\"‚è≠Ô∏è Skipping (already processed): {pdf_file}\")\n",
    "            continue\n",
    "\n",
    "        base_filename = os.path.splitext(pdf_file)[0]\n",
    "        safe_name = re.sub(r\"[^\\w\\-_. ]\", \"_\", base_filename)\n",
    "\n",
    "        print(f\"üìÑ Processing: {pdf_file}...\")\n",
    "\n",
    "        try:\n",
    "            markdown = extract_pdf_as_markdown(pdf_path, image_dir)\n",
    "            md_path = os.path.join(output_dir, f\"{safe_name}.md\")\n",
    "\n",
    "            with open(md_path, \"w\", encoding=\"utf-8\") as md_file:\n",
    "                md_file.write(markdown)\n",
    "\n",
    "            print(f\"‚úÖ Markdown saved: {md_path}\")\n",
    "            new_hashes.add(pdf_digest)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to process {pdf_file}: {e}\")\n",
    "\n",
    "    # Save updated hash store\n",
    "    all_hashes = existing_hashes.union(new_hashes)\n",
    "    save_pdf_hashes(all_hashes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "15369c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_markdown_files(\n",
    "    markdown_dir: str = \"./markdowns\",\n",
    "    combined_path: str = COMBINED_MD_PATH,\n",
    "    hash_store: str = COMBINED_MD_HASH_STORE,\n",
    "):\n",
    "    \"\"\"\n",
    "    Concatenate all *.md files in `markdown_dir` into one file\n",
    "    (`combined_path`) and store a hash based on their contents in `hash_store`.\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(combined_path), exist_ok=True)\n",
    "\n",
    "    # 1. Gather and sort all markdown files\n",
    "    md_files = sorted(\n",
    "        f for f in os.listdir(markdown_dir)\n",
    "        if f.endswith(\".md\") and f != os.path.basename(combined_path)\n",
    "    )\n",
    "\n",
    "    combined_raw = \"\"\n",
    "    combined_fingerprint = \"\"\n",
    "\n",
    "    for fname in md_files:\n",
    "        fpath = os.path.join(markdown_dir, fname)\n",
    "        with open(fpath, \"r\", encoding=\"utf-8\") as f:\n",
    "            content = f.read().strip()\n",
    "        combined_raw += (\n",
    "            f\"\\n\\n# --- Start of: {fname} ---\\n\\n\"\n",
    "            f\"{content}\\n\\n\"\n",
    "            f\"# --- End of: {fname} ---\\n\"\n",
    "        )\n",
    "\n",
    "        # Fingerprint includes filename + content hash\n",
    "        file_hash = hashlib.sha256(content.encode(\"utf-8\")).hexdigest()\n",
    "        combined_fingerprint += f\"{fname}:{file_hash}\\n\"\n",
    "\n",
    "    # Compute the final fingerprint hash\n",
    "    fingerprint_hash = hashlib.sha256(combined_fingerprint.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "    # Check for change\n",
    "    if os.path.exists(hash_store):\n",
    "        with open(hash_store, \"r\") as f:\n",
    "            old_hash = f.read().strip()\n",
    "        if old_hash == fingerprint_hash:\n",
    "            print(\"‚úÖ combined.md unchanged; skipping update.\")\n",
    "            return\n",
    "\n",
    "    # Write combined markdown\n",
    "    with open(combined_path, \"w\", encoding=\"utf-8\") as out:\n",
    "        out.write(combined_raw.strip())\n",
    "\n",
    "    # Save new fingerprint hash\n",
    "    with open(hash_store, \"w\") as f:\n",
    "        f.write(fingerprint_hash)\n",
    "\n",
    "    print(f\"‚úÖ Combined markdown updated: {combined_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "847c9ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_faiss_index(store, index_path=FAISS_INDEX_PATH):\n",
    "    store.save_local(index_path)\n",
    "\n",
    "def load_faiss_index(embeddings, index_path=FAISS_INDEX_PATH):\n",
    "    return FAISS.load_local(index_path, embeddings, allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "889af163",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_or_update_vector_store(embeddings, force=False):\n",
    "    if not force:\n",
    "        # Check if combined.md hash has changed\n",
    "        if os.path.exists(FAISS_INDEX_PATH) and os.path.exists(METADATA_STORE_PATH) and os.path.exists(COMBINED_MD_HASH_STORE):\n",
    "            print(\"üì¶ FAISS is up-to-date. Skipping rebuild.\")\n",
    "            return load_faiss_index(embeddings, FAISS_INDEX_PATH)\n",
    "\n",
    "    print(\"üîç Reading combined markdown...\")\n",
    "    with open(COMBINED_MD_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "        raw_text = f.read()\n",
    "\n",
    "    # ----------- Chunking -----------\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1200,\n",
    "        chunk_overlap=300,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
    "    )\n",
    "    chunks = text_splitter.create_documents([raw_text])\n",
    "    for chunk in chunks:\n",
    "        chunk.metadata = {\"source\": \"combined.md\"}\n",
    "\n",
    "    print(\"‚öôÔ∏è Generating embeddings...\")\n",
    "    doc_embeddings = [embeddings.embed_query(doc.page_content) for doc in tqdm(chunks, desc=\"Embedding\")]\n",
    "\n",
    "    vector_store = FAISS.from_embeddings(\n",
    "        text_embeddings=[(doc.page_content, emb) for doc, emb in zip(chunks, doc_embeddings)],\n",
    "        embedding=embeddings,\n",
    "        metadatas=[doc.metadata for doc in chunks]\n",
    "    )\n",
    "\n",
    "    save_faiss_index(vector_store)\n",
    "\n",
    "    # Save updated hash\n",
    "    with open(COMBINED_MD_HASH_STORE, \"w\") as f:\n",
    "        f.write(hashlib.sha256(raw_text.encode(\"utf-8\")).hexdigest())\n",
    "\n",
    "    print(\"‚úÖ Vector store updated and saved.\")\n",
    "    return vector_store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "30d3df6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≠Ô∏è Skipping (already processed): 2020TrustFundAnnualReports.pdf\n",
      "‚è≠Ô∏è Skipping (already processed): 2021TrustFundAnnualReports.pdf\n",
      "‚è≠Ô∏è Skipping (already processed): 2022TrustFundAnnualReports.pdf\n",
      "‚è≠Ô∏è Skipping (already processed): 2023TrustFundAnnualReports.pdf\n",
      "‚è≠Ô∏è Skipping (already processed): 2024TrustFundAnnualReports.pdf\n"
     ]
    }
   ],
   "source": [
    "# Process PDFs ‚ûú Markdown\n",
    "process_all_pdfs_to_markdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8a324864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Combined markdown updated: ./combined/combined.md\n"
     ]
    }
   ],
   "source": [
    "# Combine all markdown files\n",
    "combine_markdown_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "52fa1963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ FAISS is up-to-date. Skipping rebuild.\n"
     ]
    }
   ],
   "source": [
    "# Build or load vector store\n",
    "vector_store = build_or_update_vector_store(embeddings, force=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "18fe9d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% üîç Similarity Search + Manual Answering\n",
    "def run_query(query: str, k: int = 500):\n",
    "    # Step 1: Embed query\n",
    "    query_embedding = embeddings.embed_query(query)\n",
    "\n",
    "    # Step 2: Retrieve top k chunks\n",
    "    results = vector_store.similarity_search_by_vector(query_embedding, k=k)\n",
    "\n",
    "    # print(f\"\\nüìå Top {k} chunks retrieved for query:\\n\\\"{query}\\\"\\n\" + \"=\"*60)\n",
    "    # for i, res in enumerate(results, 1):\n",
    "    #     print(f\"\\nResult {i}:\\n{res.page_content}\\n{'='*60}\")\n",
    "\n",
    "    # Step 3: Assemble context from results\n",
    "    context = \"\\n\\n\".join(doc.page_content for doc in results)\n",
    "\n",
    "    # Step 4: Prompt LLM using retrieved context\n",
    "    prompt = (\n",
    "    \"You are a document analysis assistant helping users extract comprehensive, evidence-based stories from the provided documents.\\n\\n\"\n",
    "    \"Your goal is to generate well-structured responses that tell a clear and factual story based on the user's query.\\n\"\n",
    "    \"Each response should:\\n\"\n",
    "    \"- Clearly mention the **region** (country or area involved)\\n\"\n",
    "    \"- Specify the **sector** (e.g., agriculture, youth employment, private sector development)\\n\"\n",
    "    \"- Identify the **donor or funding source** (e.g., UKAID, World Bank)\\n\"\n",
    "    \"- Reference the **source document** (e.g., '2020TrustFundAnnualReports.pdf')\\n\\n\"\n",
    "    \"Structure the output as follows:\\n\"\n",
    "    \"### üìç Region: <Region>\\n\"\n",
    "    \"### üèóÔ∏è Sector: <Sector>\\n\"\n",
    "    \"### üí∞ Donor/Funding: <Donor(s)>\\n\"\n",
    "    \"### üí∞Total Spent: <Total Amount>\\n\"\n",
    "    \"### üìÑ Source: <Source File>\\n\"\n",
    "    \"### üßæ Story:\\n\"\n",
    "    \"- Begin with a 1-2 sentence summary.\\n\"\n",
    "    \"- Follow up with specific details, including dates, programs, financial figures, and outcomes.\\n\"\n",
    "    \"- Use bullet points or short paragraphs for clarity if needed.\\n\\n\"\n",
    "    \"**Only use information found in the provided context. Do not fabricate or assume anything.**\\n\\n\"\n",
    "    \"Context:\\n\"\n",
    "    f\"{context}\\n\\n\"\n",
    "    \"User Query:\\n\"\n",
    "    f\"{query}\"\n",
    ")\n",
    "\n",
    "    response = llm.invoke([HumanMessage(content=prompt)])\n",
    "    print(f\"\\nüß† Final Answer:\\n{response.content}\")\n",
    "    return response.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "96bb85c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîé Question: how much amount spent on the agriculture and food sector in bangladesh in 2020?\n",
      "\n",
      "üß† Final Answer:\n",
      "### üìç Region: Bangladesh  \n",
      "### üèóÔ∏è Sector: Agriculture and Food  \n",
      "### üí∞ Donor/Funding: World Bank Group Trust Funds (including Jobs MDTF and other associated grants)  \n",
      "### üí∞ Total Spent: Approximately $1.77 million (Grant Amount for Jobs MDTF activities in Bangladesh)  \n",
      "### üìÑ Source: 2020TrustFundAnnualReports.pdf, 2021TrustFundAnnualReports.pdf  \n",
      "\n",
      "### üßæ Story:  \n",
      "- In 2020, Bangladesh received multiple trust fund grants supporting agriculture and food sector development, particularly under the Jobs Multi-Donor Trust Fund (MDTF) and related World Bank projects.  \n",
      "- The **Livestock and Dairy Development Project (P161246)** aimed to improve productivity, market access, and resilience of smallholder farmers and agro-entrepreneurs in selected livestock value chains. Activities included developing a job measurement framework and establishing baselines for job indicators in the livestock sector.  \n",
      "- As of December 2020, the Jobs MDTF had allocated about **$1.77 million** in grants for Bangladesh across 5 grants, with **$1.34 million disbursed**. These funds supported activities such as job creation measurement, capacity building, and analytical work within agriculture and livestock sectors.  \n",
      "- The COVID-19 pandemic caused delays in some activities, but the project teams adapted by recruiting local consultants and adjusting timelines to continue progress.  \n",
      "- Other agriculture-related activities included support for private investment and digital entrepreneurship in hi-tech parks and economic zones, aiming to promote job creation in diversified manufacturing sectors linked to agriculture.  \n",
      "- The grant activities focused on improving job quality, access, and creation, particularly for women and youth, along with strengthening monitoring and evaluation systems for agriculture-related jobs.  \n",
      "- The Jobs MDTF and associated trust funds complemented broader World Bank investments and technical assistance aimed at enhancing agricultural productivity, market linkages, and resilience in Bangladesh during 2020.  \n",
      "\n",
      "**Note:** The exact total amount spent specifically on agriculture and food sector projects in Bangladesh in 2020 is not explicitly stated beyond the Jobs MDTF grant figures (~$1.77 million allocated). However, this reflects the core trust fund-supported activities documented for that year in the sector.\n"
     ]
    }
   ],
   "source": [
    "q = \"how much amount spent on the agriculture and food sector in bangladesh in 2020?\" \n",
    "print(f\"\\nüîé Question: {q}\")\n",
    "answer = run_query(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a136b9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def setup_rag_chain(embeddings, llm):\n",
    "#     # Load or update vector store\n",
    "#     vectorstore = build_or_update_vector_store(embeddings)\n",
    "#     retriever = vectorstore.as_retriever(search_kwargs={\"k\": 7})\n",
    "\n",
    "#     prompt = ChatPromptTemplate.from_messages([\n",
    "#     (\"system\", \n",
    "#         \"You are a factual assistant helping users extract **accurate, well-supported answers** from official development documents, such as annual reports and trust fund updates.\\n\\n\"\n",
    "#         \"**Always use only the provided context. Do not make up any information.**\\n\\n\"\n",
    "#         \"When possible, include:\\n\"\n",
    "#         \"- Relevant figures (amounts, dates)\\n\"\n",
    "#         \"- Sector, region, country, donor (if available)\\n\"\n",
    "#         \"- Project name or description (if applicable)\\n\"\n",
    "#         \"- Source document name\\n\\n\"\n",
    "#         \"If information is not available in the context, clearly state 'Not found in the provided context.'\\n\\n\"\n",
    "#         \"Context:\\n{context}\"),\n",
    "#     (\"human\", \"{input}\")\n",
    "#     ])\n",
    "\n",
    "\n",
    "\n",
    "#     # Chain assembly\n",
    "#     document_chain = create_stuff_documents_chain(llm=llm, prompt=prompt)\n",
    "#     rag_chain = create_retrieval_chain(retriever, document_chain)\n",
    "\n",
    "#     return rag_chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "40d96ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # === Run a Query ===\n",
    "# def run_query(question: str):\n",
    "#     rag_chain = setup_rag_chain(embeddings, llm)\n",
    "#     result = rag_chain.invoke(\n",
    "#         {\"input\": question}\n",
    "#     )\n",
    "#     return result[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1e3e172b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# q = \"how much amount spent on bangladesh in 2020?\" \n",
    "\n",
    "# print(f\"\\n {q}\")\n",
    "# answer = run_query(q)\n",
    "# print(f\"üß† {answer}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
